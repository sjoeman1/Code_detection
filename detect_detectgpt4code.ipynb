{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch, json\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from utils_batch import InfillingModel\n",
    "from torch.nn.functional import log_softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "device = \"cuda:1\" # for GPU usage or \"cpu\" for CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(human_scores, gpt_scores):\n",
    "    # Data\n",
    "    A, B = human_scores, gpt_scores\n",
    "    # Combine scores and true labels\n",
    "    scores = A + B\n",
    "    labels = [0] * len(A) + [1] * len(B)\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    # Calculate AUC (Area Under Curve)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve: Open-gen w/ GPT3.5-Reddit w prompts' )\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    # what is the TPR for FPR = 0.1?\n",
    "    for idx, fpr_ in enumerate(fpr):\n",
    "        if fpr_ > 0.1:\n",
    "            print(f\"TPR at 1% FPR: {tpr[idx]:.4f}\")\n",
    "            break\n",
    "    return roc_auc, tpr[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NinedayWang/PolyCoder-160M\n",
    "# NinedayWang/PolyCoder-0.4B\n",
    "# NinedayWang/PolyCoder-2.7B\n",
    "model_name = 'codeparrot/codeparrot'\n",
    "PyCodeGPT = AutoModelForCausalLM.from_pretrained( model_name ).to(device)\n",
    "PyCodeGPT_tokenizer = AutoTokenizer.from_pretrained( model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "# model_name = \"/data/xianjun/project/llama/7B_hf/\"\n",
    "# model = LlamaForCausalLM.from_pretrained( model_name ).half().to(device) #.half() to use FP16\n",
    "# model.eval() \n",
    "# PyCodeGPT = model\n",
    "# PyCodeGPT_tokenizer = LlamaTokenizer.from_pretrained( model_name ) #.half() to use FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "# Load the model's configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "# Get the default max_length\n",
    "max_length = config.max_position_embeddings\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give an input, return the logits of input tokens\n",
    "inputs = 'this is a test'\n",
    "truncate_ratio=0.9\n",
    "def get_logprob1(inputs ):\n",
    "    input_ids = PyCodeGPT_tokenizer.encode(inputs, return_tensors='pt').to(device)\n",
    "    input_ids = input_ids[:, :max_length]\n",
    "    with torch.no_grad():\n",
    "        output = PyCodeGPT(input_ids)\n",
    "    logits = output[0]\n",
    "    # Assuming the `logits` tensor contains the output from the model\n",
    "    log_probs = log_softmax(logits, dim=-1)\n",
    "    # Select the log probabilities for the specific tokens in the input\n",
    "    input_log_probs = log_probs[0, torch.arange(log_probs.size(1)), input_ids[0]]\n",
    "    # Multiply by -1 to get the negative log probabilities\n",
    "    neg_log_probs = -input_log_probs\n",
    "    neg_log_probs = neg_log_probs.cpu().numpy().tolist()\n",
    "    return np.average( neg_log_probs[int( truncate_ratio*len(neg_log_probs)): ] ), neg_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### merge the datasets\n",
    "with open('results/regen_gpt-3.5-turbo_20_0.5.jsonl', 'r') as f:\n",
    "    data1  = [json.loads(line) for line in f]\n",
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[0]['machine_gen_text']['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = data1[0]['machine_prefix_prompt'] + data1[0]['machine_gen_text']['choices'][0]['message']['content']\n",
    "human_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1[0]['machine_gen_text']['choices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_neg_log_probs, neg_log_probs = get_logprob1(human_text)\n",
    "neg_log_probs[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give an input, return the logits of input tokens\n",
    "truncate_ratio=0.9\n",
    "def get_logprob(inputs ):\n",
    "    input_ids = PyCodeGPT_tokenizer.encode(inputs, return_tensors='pt').to(device)\n",
    "    input_ids = input_ids[:, :max_length]\n",
    "    with torch.no_grad():\n",
    "        output = PyCodeGPT(input_ids)\n",
    "    logits = output[0]\n",
    "    # Assuming the `logits` tensor contains the output from the model\n",
    "    log_probs = log_softmax(logits, dim=-1)\n",
    "    # Select the log probabilities for the specific tokens in the input\n",
    "    input_log_probs = log_probs[0, torch.arange(log_probs.size(1)), input_ids[0]]\n",
    "    # Multiply by -1 to get the negative log probabilities\n",
    "    neg_log_probs = -input_log_probs\n",
    "    neg_log_probs = neg_log_probs.cpu().numpy().tolist()\n",
    "    return np.average( neg_log_probs[int( truncate_ratio*len(neg_log_probs)): ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_prob_all = []\n",
    "for id, ins in tqdm.tqdm(enumerate(data1), total=len(data1)):\n",
    "    temp = []\n",
    "    if len( ins['human_gen_text']['choices'] ) > 1:\n",
    "        original_score = get_logprob( ins['gold_completion'] ) - get_logprob( ins['human_prefix_prompt'] )\n",
    "        miu_scores = []\n",
    "        for i in range( len(ins['human_gen_text']['choices'] ) ):\n",
    "            one_regen = ins['human_prefix_prompt'] + ins['human_gen_text']['choices'][i]['message']['content']\n",
    "            miu_scores.append( get_logprob( one_regen ) - get_logprob( ins['human_prefix_prompt'] ) )\n",
    "        miu_scores_average_score = np.average( miu_scores )\n",
    "        dx = original_score - miu_scores_average_score\n",
    "        gold_prob_all.append( dx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "fim_prob_all = []\n",
    "for id, ins in tqdm.tqdm(enumerate(data1), total=len(data1)):\n",
    "    ins = data1[id]\n",
    "    miu_scores = []\n",
    "    original_score = get_logprob( ins['parsed_completion'] ) - get_logprob( ins['machine_prefix_prompt'])\n",
    "    for i in range( len(ins['machine_gen_text']['choices'] ) ):\n",
    "        one_regen = ins['machine_prefix_prompt'] + ins['machine_gen_text']['choices'][i]['message']['content']\n",
    "        miu_scores.append( get_logprob( one_regen  ) - get_logprob( ins['machine_prefix_prompt'] ) )\n",
    "    miu_scores_average_score = np.average( miu_scores )\n",
    "    dx = original_score - miu_scores_average_score\n",
    "    fim_prob_all.append( dx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and give different colors\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gold_prob_all, label='human')\n",
    "plt.plot(fim_prob_all, label='gpt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve( gold_prob_all, fim_prob_all  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
